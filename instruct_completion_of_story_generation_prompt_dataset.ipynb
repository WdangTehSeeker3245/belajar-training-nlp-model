{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L44e1OhYJqy9"
      },
      "source": [
        "**Using Falcon**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VArKuq9kJW4W"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets huggingface_hub bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx5TAp3HLEBU"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"faizalnf1800/gpt3.5-scifi-story-prompt\")\n",
        "dataset = dataset['train']['prompt'][:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v7ea1W2Mrdv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer,BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    load_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"ericzzz/falcon-rw-1b-instruct-openorca\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Move the model to GPU\n",
        "# device = 0\n",
        "# model.to(device)\n",
        "\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Create a pipeline with the model and tokenizer on GPU\n",
        "# generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Initialize lists to store prompts and completions\n",
        "prompts = []\n",
        "completions = []\n",
        "\n",
        "# Inference loop\n",
        "for i in range(len(dataset)):\n",
        "    # Input text for generation\n",
        "    prompt = dataset[i]\n",
        "\n",
        "    # Generate text using the pipeline with additional parameters\n",
        "    generated_text = generator(\n",
        "        prompt,\n",
        "        max_length=3000,\n",
        "        min_length=1000,\n",
        "        num_return_sequences=1,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=2.0,\n",
        "        repetition_penalty=2.0,\n",
        "        num_beams=5,\n",
        "        length_penalty=0.8,\n",
        "        pad_token_id=generator.tokenizer.eos_token_id,\n",
        "        do_sample=True\n",
        "    )[0]['generated_text']\n",
        "\n",
        "    # Remove the prompt from the generated text\n",
        "    generated_text = generated_text.replace(prompt, \"\").strip()\n",
        "\n",
        "    # Ensure the length is within the desired range\n",
        "    generated_text = generated_text\n",
        "\n",
        "    # Append prompt and generated text to the lists\n",
        "    prompts.append(prompt)\n",
        "    completions.append(generated_text)\n",
        "\n",
        "    # Print the prompt and generated text\n",
        "    print(f\"Generated Text loop {i}:\\n {generated_text}\\n\")\n",
        "\n",
        "# Create a DataFrame from the lists\n",
        "df = pd.DataFrame({'prompt': prompts, 'completion': completions})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('falcon-1b-instruct-completion-scifi-story-prompt.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwQwusL1n549"
      },
      "outputs": [],
      "source": [
        "# Free up CPU memory\n",
        "!python3 -c \"import psutil; psutil.virtual_memory().percent = 0\"\n",
        "\n",
        "# Free up GPU memory\n",
        "!nvidia-smi -l | grep Memory | awk '{print $2}' | xargs sudo shuf -n1 /dev/null"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS-yfleN6IK9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch ipython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zdhtJMbe9ieY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from IPython.display import display, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZxdbfhsfbMw_"
      },
      "outputs": [],
      "source": [
        "precision = 'half' # @param [\"half\", \"full\"]\n",
        "if precision == \"half\":\n",
        "    selected_precision = torch.float16\n",
        "elif precision == \"full\":\n",
        "    selected_precision = torch.float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fS7L9e-pbMxF"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czXanMio-f9H"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", torch_dtype=selected_precision)\n",
        "model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "h7MJFSlJbMxJ"
      },
      "outputs": [],
      "source": [
        "# @title Hyperparameter Inference\n",
        "max_length = 200 # @param {type:\"integer\"}\n",
        "min_length = 200 # @param {type:\"integer\"}\n",
        "top_k=70 # @param {type:\"integer\"}\n",
        "top_p = 1.2 # @param {type:\"number\"}\n",
        "temperature=2.0 # @param {type:\"number\"}\n",
        "repetition_penalty=2.0 # @param {type:\"number\"}\n",
        "do_sample=True # @param {type:\"boolean\"}\n",
        "use_cache=True # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1xZcnKx5Txd"
      },
      "outputs": [],
      "source": [
        "prompt = \"Once upon a time\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\n",
        "output = model.generate(\n",
        "                        inputs[\"input_ids\"],\n",
        "                        max_length=max_length,\n",
        "                        min_length=min_length,\n",
        "                        # num_return_sequences=1,\n",
        "                        top_k=top_k,\n",
        "                        top_p=top_p,\n",
        "                        temperature=temperature,\n",
        "                        repetition_penalty=repetition_penalty,\n",
        "                        # num_beams=5,\n",
        "                        # length_penalty=0.8,\n",
        "                        # penalty_alpha=0.5,\n",
        "                        do_sample=True,\n",
        "                        use_cache=True\n",
        "                        )\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HTML and CSS for displaying the output in a square box\n",
        "html_code = f\"\"\"\n",
        "<div style='border: 2px solid black; padding: 10px; width: 300px; height: 300px; overflow: auto;'>\n",
        "    <p style='font-family: monospace;'>{decoded_output}</p>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "# Display the styled output\n",
        "display(HTML(html_code))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "eukuIxnpcTdF",
        "outputId": "3fbd03f4-49d1-4069-ad61-482e77405bf3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style='border: 2px solid black; padding: 10px; width: 300px; height: 300px; overflow: auto;'>\n",
              "    <p style='font-family: monospace;'>Once upon a time all the people known are saying is why haven't this had sex but... what am we suppose to do that he'll feel more comfortable being with.... then it does like, come get your husband next minute..... because how will an adult who knew nothing need satisfy their desire..\n",
              "Unusual request after usual \"how might...\" (well donï¿½t play around I promise if you call them mean or even sad)... Oh oh its very kind and not fun at one period please relax my love! What shall have become. All i wanted was... It'd make for easier stuff so stop teasing later thank heaven! Thankyou mate /r4tt2 :) http://piiurlforgeinfousercontent203615521710/d11jwz3mr25QnkmnAuN5LhDmyaViE1nwlIeHQ0AkG9sMckSki-BtcRYvww 4 1 Nuv</p>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
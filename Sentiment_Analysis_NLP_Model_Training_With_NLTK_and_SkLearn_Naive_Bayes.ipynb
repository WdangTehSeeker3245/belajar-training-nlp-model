{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Task: Develop a Natural Language Processing (NLP) Model for Sentiment Analysis**\n",
        "\n",
        "**Description:**\n",
        "You are tasked with building a machine learning model for sentiment analysis. The goal is to create a model that can classify text data (such as product reviews or social media posts) into different sentiment categories, such as positive, negative, or neutral. This model will be used to automatically assess the sentiment of customer reviews for a product.\n",
        "\n",
        "**Steps and Responsibilities:**\n",
        "\n",
        "1. **Data Collection:** Gather a dataset of text samples with labeled sentiment. This dataset should include a variety of text sources and cover different domains relevant to the product.\n",
        "\n",
        "2. **Data Preprocessing:** Clean and preprocess the text data. This may involve tasks such as tokenization, removing stop words, stemming or lemmatization, and handling any missing data.\n",
        "\n",
        "3. **Feature Engineering:** Convert the text data into numerical features that can be used as input for the machine learning model. Common techniques include TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings like Word2Vec or GloVe.\n",
        "\n",
        "4. **Model Selection:** Choose an appropriate machine learning or deep learning model for sentiment analysis. Common choices include logistic regression, support vector machines, recurrent neural networks (RNNs), or transformer models like BERT.\n",
        "\n",
        "5. **Model Training:** Train the selected model using the preprocessed data. This involves splitting the data into training and validation sets, setting hyperparameters, and training the model to minimize the loss function.\n",
        "\n",
        "6. **Evaluation:** Assess the model's performance using appropriate metrics like accuracy, precision, recall, F1-score, or ROC-AUC. Fine-tune the model and hyperparameters as needed.\n",
        "\n",
        "7. **Testing and Deployment:** After achieving a satisfactory level of performance, test the model on an independent test dataset to ensure generalization. Once validated, deploy the model to a production environment, which may involve creating APIs or integrating it into a larger system.\n",
        "\n",
        "8. **Monitoring and Maintenance:** Continuously monitor the model's performance in the production environment, as well as the data it encounters. Implement regular model retraining to account for concept drift and changing data distributions.\n",
        "\n",
        "9. **Documentation:** Maintain documentation of the entire process, including data sources, preprocessing steps, model architecture, hyperparameters, and evaluation results.\n",
        "\n",
        "10. **Scaling and Optimization:** As the application grows, consider optimization techniques like distributed computing, parallel processing, and model compression to improve efficiency and scalability.\n",
        "\n",
        "11. **Feedback Loop:** Gather user feedback on the model's predictions and iteratively improve it based on real-world performance.\n",
        "\n",
        "This task showcases a common responsibility of an AI and ML engineer, which is to develop, deploy, and maintain machine learning models for specific applications. The specific tasks and tools used may vary depending on the project and its requirements.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5wkU_953MMUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "VOzElWcuxL4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"yelp_review_full\")"
      ],
      "metadata": {
        "id": "Vex_eKiJlKC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tqdm import tqdm\n",
        "import nltk"
      ],
      "metadata": {
        "id": "Vq81a2EUzQmu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your dataset has columns 'label' and 'text'\n",
        "df_training = pd.DataFrame({'text': dataset['train']['text'], 'label': dataset['train']['label']})\n",
        "df_test = pd.DataFrame({'text': dataset['test']['text'], 'label': dataset['test']['label']})\n",
        "print(df_training.count())\n",
        "print(df_test.count())"
      ],
      "metadata": {
        "id": "0kaM0NLc0cqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "oFaoEKya0_Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define the preprocess_text function\n",
        "def preprocess_text_with_progress(text):\n",
        "    words = word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha()]\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "U56HBS0m0gHe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_training['text'] = df_training['text'].progress_apply(preprocess_text_with_progress)"
      ],
      "metadata": {
        "id": "ZGEcoZ6BvTEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['text'] = df_test['text'].progress_apply(preprocess_text_with_progress)"
      ],
      "metadata": {
        "id": "NpztjWDcIAEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_training.to_csv('training_data.csv', index=False)\n",
        "df_test.to_csv('test_data.csv', index=False)"
      ],
      "metadata": {
        "id": "69MuKg73Pam7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_files = {\"train\": \"training_data.csv\", \"test\": \"test_data.csv\"}\n",
        "preprocessed_dataset = load_dataset(\"csv\", data_files=data_files)"
      ],
      "metadata": {
        "id": "0FQkd-y-Pb1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "05GFBlwx6exP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "gvAUM2QE6i-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_dataset.push_to_hub(\"faizalnf1800/yelp_review_full_preprocessed\")"
      ],
      "metadata": {
        "id": "XTZ0oa_oPjY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset2 = load_dataset(\"faizalnf1800/yelp_review_full_preprocessed\")"
      ],
      "metadata": {
        "id": "c8IUGs0LPnW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_training2 = pd.DataFrame({'text': dataset2['train']['text'], 'label': dataset2['train']['label']})"
      ],
      "metadata": {
        "id": "b6_ARRkLP3JU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test2 = pd.DataFrame({'text': dataset2['test']['text'], 'label': dataset2['test']['label']})"
      ],
      "metadata": {
        "id": "YLpich7ERSFe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(lowercase=False)"
      ],
      "metadata": {
        "id": "vgrSWBE12jol"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_training2['text'] = df_training2['text'].astype(str)\n",
        "df_test2['text'] = df_test2['text'].astype(str)"
      ],
      "metadata": {
        "id": "ArtQudSSQDZ8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and transform on training data\n",
        "X_train = tfidf_vectorizer.fit_transform(df_training2['text'])\n",
        "y_train = df_training2['label']\n",
        "\n",
        "# Transform the test data using the same vectorizer\n",
        "X_test = tfidf_vectorizer.transform(df_test2['text'])\n",
        "y_test = df_test2['label']"
      ],
      "metadata": {
        "id": "zr0hzBm_wWYy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and train the model (Naive Bayes)\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "OlV9Av3s2vtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "-pfj9WR52_hl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(report)"
      ],
      "metadata": {
        "id": "hGB50L3t3GTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text to classify\n",
        "# neutral\n",
        "#input_text = \"Overall, the product meets my expectations. It does what it's supposed to do, but I wouldn't say it's outstanding. It's an average product with no major complaints. It gets the job done.\"\n",
        "# positive\n",
        "#input_text = \"I'm absolutely thrilled with this product! It exceeded my expectations in every way. The quality is top-notch, and it's incredibly easy to use. I've been using it for a while now, and it has made my life so much better. I highly recommend it to anyone looking for a reliable and efficient solution.\"\n",
        "# negative\n",
        "input_text = \"I'm really disappointed with this product. It didn't work as advertised, and I encountered numerous issues from the moment I started using it. The quality is subpar, and it's a waste of money. I regret purchasing it and wouldn't recommend it to anyone.\"\n",
        "\n",
        "# Preprocess the input text\n",
        "input_text = preprocess_text_with_progress(input_text)\n",
        "\n",
        "# Vectorize the input text using the same TF-IDF vectorizer\n",
        "input_vector = tfidf_vectorizer.transform([input_text])\n",
        "\n",
        "# Make predictions\n",
        "predicted_label = model.predict(input_vector)[0]\n",
        "\n",
        "# Print the result\n",
        "if (predicted_label > 2) :\n",
        "    print(\"Positive Sentiment\")\n",
        "elif (predicted_label == 2):\n",
        "    print(\"Neutral Sentiment\")\n",
        "else:\n",
        "    print(\"Negative Sentiment\")\n",
        "\n",
        "print(predicted_label)"
      ],
      "metadata": {
        "id": "thHACU9u3M7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir sentiment_scoring"
      ],
      "metadata": {
        "id": "gb4SmC3hHqN2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save your NLTK model\n",
        "with open('/content/sentiment_scoring/sentiment-scoring-nltk-sklearn-naivebayes.pkl', 'wb') as model_file:\n",
        "    pickle.dump(model, model_file)"
      ],
      "metadata": {
        "id": "yZDtoPcB5jtM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli upload faizalnf1800/sentiment-scoring-nltk-sklearn-naivebayes /content/sentiment_scoring/sentiment-scoring-nltk-sklearn-naivebayes.pkl sentiment-scoring-nltk-sklearn-naivebayes.pkl"
      ],
      "metadata": {
        "id": "TiPLG_nLEuBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "hf_hub_download(repo_id=\"faizalnf1800/sentiment-scoring-nltk-sklearn-naivebayes\", filename=\"sentiment-scoring-nltk-sklearn-naivebayes.pkl\", local_dir=\"/content\")"
      ],
      "metadata": {
        "id": "4q_LfwbHHyBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickled_model = pickle.load(open('sentiment-scoring-nltk-sklearn-naivebayes.pkl', 'rb'))\n",
        "\n",
        "# Sample text to classify\n",
        "# neutral\n",
        "#input_text = \"Overall, the product meets my expectations. It does what it's supposed to do, but I wouldn't say it's outstanding. It's an average product with no major complaints. It gets the job done.\"\n",
        "# positive\n",
        "#input_text = \"I'm absolutely thrilled with this product! It exceeded my expectations in every way. The quality is top-notch, and it's incredibly easy to use. I've been using it for a while now, and it has made my life so much better. I highly recommend it to anyone looking for a reliable and efficient solution.\"\n",
        "# negative\n",
        "input_text = \"I'm really disappointed with this product. It didn't work as advertised, and I encountered numerous issues from the moment I started using it. The quality is subpar, and it's a waste of money. I regret purchasing it and wouldn't recommend it to anyone.\"\n",
        "\n",
        "\n",
        "# Preprocess the input text\n",
        "input_text = preprocess_text_with_progress(input_text)\n",
        "\n",
        "# Vectorize the input text using the same TF-IDF vectorizer\n",
        "input_vector = tfidf_vectorizer.transform([input_text])\n",
        "# Make predictions\n",
        "predicted_label = pickled_model.predict(input_vector)[0]\n",
        "\n",
        "# Print the result\n",
        "if (predicted_label > 2) :\n",
        "    print(\"Positive Sentiment\")\n",
        "elif (predicted_label == 2):\n",
        "    print(\"Neutral Sentiment\")\n",
        "else:\n",
        "    print(\"Negative Sentiment\")\n",
        "\n",
        "print(predicted_label)"
      ],
      "metadata": {
        "id": "TnWSLONQL_g2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}